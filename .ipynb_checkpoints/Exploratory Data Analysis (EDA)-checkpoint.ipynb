{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Multicolliniarity</h1>\n",
    "<ul>\n",
    "    <li>What causes multicollinearity?</li>\n",
    "    <ul>\n",
    "        <li>Linear dependency between variables</li>\n",
    "        <li>Overlapping variables</li>\n",
    "        <li>Data transformation</li>\n",
    "        <li>Data collection</li>\n",
    "        <li>Small sample size</li>\n",
    "    </ul>\n",
    "    <li>What is multicolliniarity?</li>\n",
    "    <ul>\n",
    "        <li>In multicollinearity, the regression coefficients are still consistent but are no longer reliable    \n",
    "            since the standard errors are inflated. It means that the modelâ€™s predictive power is not reduced, \n",
    "            but the coefficients may not be statistically significant with a Type II error.</li>\n",
    "        <li>It refers to the presence of high correlation between predictor variables</li>\n",
    "        <li>Not a problem for non-linear regression methods such as trees, nearest neighbors, clustering\n",
    "    </ul>\n",
    "    <li>How to check/measure for multicollinariarity?</li>\n",
    "    <ul>\n",
    "        <li>Correlation matrix</li>\n",
    "        <li>Eigenvalues of the correlation matrix or the covariance matrix: If one or more eigen values are close \n",
    "            to zero, it indicates multicolliarity</li>\n",
    "        <li>Variance Inflection Factor (VIF): It measures the extend to which the variance of the estimated \n",
    "            regresssion coefficient is increased due to multicollinarity</li> \n",
    "        <li>A VIF>5 or 10 suggest high multicolliniarity (compute VIF for each predictor variable)</li> \n",
    "        <li>VIF=1 no correlation between independent variables and other variables</li>$\\text{VIF}(\\hat{\\beta}_j) = \\frac{1}{1 - R_j^2}$\n",
    "        <p>In the above formula, VIF(&Hat;&beta;<sub>j</sub>) represents the Variance Inflation Factor for the \n",
    "            jth predictor variable, &Hat;&beta;<sub>j</sub> represents the estimated coefficient for the jth \n",
    "            predictor in the regression model, and R<sub>j</sub><sup>2</sup> represents the coefficient of \n",
    "            determination (R-squared) for the regression model with the jth predictor as the dependent variable \n",
    "        </p>\n",
    "        <li>Condition number: A large condition number suggest potential multicollinearity</li>\n",
    "    </ul>\n",
    "    <li>How to fix mmulticollinearity?</li>\n",
    "    <ul>\n",
    "        <li>Remove correlated variables: Identify the variables that are highly correlated with each other and \n",
    "            remove one of them from the analysis</li>\n",
    "        <li>Feature selection: Instead of removing variables completely, you can use feature selection techniques \n",
    "            to choose a subset of relevant and uncorrelated variables</li>\n",
    "        <li>Combine variables: Instead of using highly correlated variables separately, consider creating new \n",
    "            variables through transformations or combining them into composite variables</li>\n",
    "       <li>Data collection: If high multicollinearity persists even after variable manipulation, consider \n",
    "           collecting additional data to provide a more diverse range of values for the variables. Increasing the \n",
    "           sample size can help reduce the impact of multicollinearity.</li>\n",
    "        <li>Regularization techniques: Utilize regularization methods like Ridge regression or Lasso regression, \n",
    "            which introduce a penalty term to the regression equation to reduce the influence of correlated \n",
    "            variables</li>\n",
    "        <li>Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that can be used to \n",
    "            create orthogonal variables called principal components. These components capture the most \n",
    "            significant variance \n",
    "            in the data while minimizing multicollinearity</li> \n",
    "        <li>Partial-Least-Square (PLS): PLS regression can reduce the variables to a smaller set with no \n",
    "            correlation among them</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        <li>Assess model performance: After addressing multicollinearity, evaluate the performance and stability of the regression model. Examine the coefficients, standard errors, p-values, and overall model fit to ensure that the results are meaningful and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>Residuals</b>\n",
    "    <li>histogram plot will help vizualize the distribution</li>\n",
    "    <li>Normal Q-Q plot: if the points fits into a line then the residuals are normally distributed, otherwise, \n",
    "        check the correlation</li>\n",
    "    </ul>\n",
    "    <li>Box-plot: it displays, min/max, median, first/thid quartile, and outlier values. It is useful to identify \n",
    "    outliers, spread, and skwness</li>\n",
    "    <li>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

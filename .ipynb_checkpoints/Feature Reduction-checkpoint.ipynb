{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Principal Component Analysis (PCA)</h1>\n",
    "<ul>\n",
    "    <li><b>Notes</b>\n",
    "        <ul>\n",
    "            <li>Is a linear combination of the predictor variables</li>\n",
    "            <li>Can't be used with categorical data (use correspondance analysis instead)</li>\n",
    "            <li>minimizes correlation between components (reducing redundancy)</li>\n",
    "            <li>A limited number of components between will explain most of the variance</li>\n",
    "            <li>Identifies principal direction of variation</li>\n",
    "            <li>reduces the number of features</li>\n",
    "            <li>good to vizualize the data</li>\n",
    "            <li>PCA will capture the data into lower dimension and retain the max variance in the data</li>\n",
    "            <li>Less reliable for small samples - needs a lot of data for the correlations to be strong</li>\n",
    "            <li>It's generally non-parametric (if the data is normally distributed the solution is enhaced)</li>\n",
    "            <li>The analysis degreades when the linearity fails</li>\n",
    "            <li>Principal component is degreaded when linearity fails</li>\n",
    "            <li>Principal components are orthogonal (dot product of two vectors is zero and are perpendicular)\n",
    "            </li>\n",
    "    </ul>\n",
    "    <li><b>Assumptions</b>\n",
    "        <ul>\n",
    "            <li>linear relationship with dependent variable </li>\n",
    "            <li>Homescedasticity: variance of the independent variables is equal accross all classes</li>\n",
    "            <li>WEAK multicollinearity</li>\n",
    "            <li>Normality</li>\n",
    "            <li>linearity</li>\n",
    "            <li>variable scales</li>\n",
    "            <li>linearly independent</li>\n",
    "            <li>follow a multivariate normal distribution</li>\n",
    "        </ul>\n",
    "    <li><b>Steps</b>\n",
    "        <ol>\n",
    "            <li>Recenter the dataset to origin (substract the mean for each column) and standarize it (matrix Z)\n",
    "            </li>\n",
    "            <li>Get covariance of matrix Z</li>\n",
    "            <li>Calculate eigen vectors and eigen values of matrix Z</li>\n",
    "            <li>Sort the eigen vectors</li>\n",
    "            <li>Calculate the variance of each vector by </li>$\\frac{\\lambda}{total \\lambda}$\n",
    "            <li>Drop unimportatn features</li>\n",
    "            <li>Fin new transformation by multiplying matrix_Z * X = y</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linearly Discriminant Analysis (LDA)</h1>\n",
    "\n",
    "<ul>\n",
    "    <li>It is like PCA but focuses on maximizing the separability among known categories</li>\n",
    "    <li>Similarities with PCA</li>\n",
    "    <ul>\n",
    "        <li>Rank the new axes in order of importance</li>\n",
    "        <li>Reduces dimensions</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
